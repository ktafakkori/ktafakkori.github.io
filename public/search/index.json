[{"content":"Introduction This article can serve as a comprehensive reference for practitioners, managers, consultants, academics, and experts to get familiar with use cases of operations research (OR) in different fields, such as industrial engineering (IE), computer science (CS), machine learning (ML), simulation (SM), and data science (DS).\nYou may not realize that there are over 1800 classes of decision problems that can be modeled using OR science both in the real world and in the virtual world. There is no doubt that the number of these classes can be more if we consider more specific practical, theoretical, or mathematical aspects. The theoretical categories of decision problems in OR can be created using the following features:\nOnline or offline Permanent, strategic, tactical, operational, rolling horizon, or real-time Deterministic or uncertain No-objective, single-objective with a linear function, single-objective with a nonlinear function, multi-objective with linear functions, or multi-objective with nonlinear functions Continuous, discrete, or mixed decision variables Unconstrained, single linear constraint, single nonlinear constraint, multiple linear constraints, or multiple nonlinear constraints Notably, due to the diverse nature of OR\u0026rsquo;s use cases, practical categories and use cases are difficult to categorize or measure. Therefore, this article will allude to keywords that can shed light on how decisions made by OR might be useful for a particular industry.\nReal-world use cases Facilities management Assortment planning Capacitated \u0026amp; un-capacitated facilities location Capacitated multi-source Weber location Capacity and expansion planning Center location Center location-dependent relocation Charging/refueling station location Closed-loop supply chain network design Competitive location Continuous location Covering location Cross-docking location Desirable \u0026amp; undesirable facilities location Discrete location Distribution network design Dynamic \u0026amp; static location Fixed-charge facility location Forward and reverse (backward) logistics network design Gateway location Hierarchical \u0026amp; single-level location Hub and spoke \u0026amp; terminal location Land use Layout and re-layout Location and demand allocation Location problem with barriers Locker location Logistics distribution centers location Minimax location Minisum location Mobile location Network design Obnoxious facilities Obstacle location-allocation Online location problem Open-loop supply chain network design P-median location Pick up/drop off location Planar, network, \u0026amp; discrete location Plant location Procurement (supply or vendor) network design Production network design Smart grid Staffing and human resources Supply chain facilities location Supply chain network design Supply chain network redesign Territory design Tree \u0026amp; general graph location Warehouse location Inventory management Backlog level Growing and perishable items Holding level Leadtime Lost sales level Order \u0026amp; reorder point Order/production quantity Placement \u0026amp; assortment Self-/vendor-managed Transportation \u0026amp; logistics management Ambulance location Bike sharing Capacitated vehicle routing Crowd logistics and crowd-shipping Dynamic traffic assignment Electric vehicles and mobility Emergency logistics First, middle, and last mile delivery Freight transportation Third, forth, \u0026amp; fifth-party logistics Intelligent transport systems Milk run and automobile Multi-agent system Network design Port smart logistics Pricing in transport networks Ridesharing School bus routing Smart agro-logistics Smart logistics Traffic enforcement systems Traffic planning Trailer rebalancing Truck-load Vehicle routing Financial management Budget allocation Capital investment Cashflow management Payment and compensation Pricing and revenue management Profit maximization Stocks investment Sourcing management Supplier development Supplier evaluation Supplier excellence Supplier relationship Supplier scoring Supplier selection and order allocation Information \u0026amp; computing management Cloud computing Control systems monitoring DRP systems Edge computing ERP systems Fog computing ICT systems Knowledge management MIS systems MRP systems MPS systems RFID Traceability, tractability, and visibility Demand management Assembly-to-order Build-to-order Call center Customer relationship Discounts, incentives, and motivations Engineer-to-order Engineer-to-order Make-to-order Make-to-stock Mobility-on-demand Negotiations On-demand production Order splitting Returns Time slot Time window Project management Planning Resource assignment Resource leveling Scheduling Technology management Additive manufacturing adoption Artificial intelligence adoption Blockchain adoption Digital transformation Internet of things adoption Modern ICT networks adoptions Sensors Industry management Agri-food and poultry Airlines Atm location planning Bio-refinery, bio-mass, and petroleum Blood and organ transplant network Broadband networks Coal blending Emergency Foreign trade Hazardous waste Health care Humanitarian and relief, and disaster Metropolitan/city logistics Movie production Operating room Pharmaceutical Port governance Portfolio and stocks Private \u0026amp; public sector Public school districting Refinery operations Renewable energy Scheduling bank staff Sports league Talent marketplaces Taxi Textile Business managemnet Advertising Continuity management Contracts Distribution marketing Flexibility and variability High-impact matching Intelligent, e-business and e-marketing Leadership, organizational culture \u0026amp; trust Leanness, agility, and leagility Risk management Paradigm management Adaptive Agile Disruption Economical Efficient Environmental Flexible Green Industry 4.0 Industry 5.0 Leagile Lean Net-zero Reliable Resilient Responsible Responsive Robust Social Sustainable Variable Production \u0026amp; schedule management Airline crew scheduling Assembly line balancing Bin packing Bio-inspired manufacturing Commodities DNA sequence assembly Fast-moving consumer goods Flow shop scheduling Human-robot collaboration Job shop scheduling Lifecycle analysis Nesting Open shop scheduling Packaging Parallel machine scheduling Perishable products Queuing Shift scheduling Single machine scheduling Virtual-world use cases Belief theory Complexity theory Data envelopment analysis Game theory Graph theory Machine learning Multi-criteria decision making Uncertainty programming ","date":"2023-03-05T00:00:00Z","image":"https://ktafakkori.github.io/use-cases-of-operations-research-update-2023/post-image_hued1f519c0727ccd3a00c1c4bfdf5b454_91037_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/use-cases-of-operations-research-update-2023/","title":"Use cases of operations research: Update 2023"},{"content":"Most of the heuristic optimization algorithms proposed are repetitive or follow the same principle/structure.\nNew research in the heuristic optimization field is often fastly criticized due to some weak studies in the field. According to Sörensen (2013), while truly innovative research of high quality is STILL being performed, multiple reasons do exist that make a new contribution worthless:\nUsing a different word for the same old concept (renaming), leading to the reiteration of existing knowledge. To avoid this, we should not change the standard vocabulary of this context. It is better to separate our \u0026ldquo;inspiration\u0026rdquo; from \u0026ldquo;theoretical algorithm development\u0026rdquo; or the report of results and discussions\u0026quot; to avoid this. For instance, through the stages of algorithm development, use \u0026ldquo;search agent\u0026rdquo; or \u0026ldquo;solution\u0026rdquo; instead of the metaphor elements (e.g., crow, harmony, \u0026hellip;). Similarly, use \u0026ldquo;better objective value (fitness)\u0026rdquo;, \u0026ldquo;decision variable\u0026rdquo;, and so on. It not only avoids repetition of existing knowledge but helps to understand the algorithm easier. Developing similar search operators even though the metaphor is different. This case is similar to when we apply the same approach to a different situation. If so, we must compare the results where possible. Accordingly, we should not leave the readers to find the linkage of our approach with previous studies. Not explaining similarities if the metaphor does the same behavior as the others or other studies. For instance, if the behavior of a \u0026ldquo;novel\u0026rdquo; meta-heuristic algorithm is similar to a reinforcement learning algorithm, we should state and explain this. In summary, the paper of Sörensen (2013), alludes to a necessary research principle, which is \u0026ldquo;we should always rigorously position our research in the literature by fair unbiased comparisons\u0026rdquo;. We should not be obsessed to propose a novel method or multiple contributions, but more be focused on links. For instance, we should also show similarities to our method/approach and try not to focus on showing our contribution is way better. If a contribution is really novel, we should strongly prove or demonstrate it by rigorous comparisons. For instance, by statistical validations or well-known test problems.\nHerein, I summarize a high-quality algorithm development instruction discussed by Sörensen (2013): Adequately framing a method entails explaining it using general optimization terminology (not metaphor-based language):\ndeconstructing it and showing its components determining similar components with previous studies (if any) how these components are adapted to (or do work in) the optimization process transparently doing parameter-tuning (if any). ","date":"2022-11-25T00:00:00Z","image":"https://ktafakkori.github.io/why-is-novel-metaheuristic-algorithm-development-often-criticized/post-image_hu5192c9b5379c75b03b3dbb1e1b4bd917_579754_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/why-is-novel-metaheuristic-algorithm-development-often-criticized/","title":"Why is novel metaheuristic algorithm development often criticized?"},{"content":"Introduction This blog post presents a list of academic tools for research, publication, sharing, citation, journal suggestion, validation, plagiarism checking, citation network analysis and more!\nSearch databases Name Link acm digital library Official aminer Official base Official citeseerx Official core Official crossref Official dblp Official deepdyve Official dimensions Official doaj Official doi Official ebsco Official google scholar Official ingenta Official jgate Official mathscinet Official mendeley Official microsoft academic search Official my science work Official orcid Official paperity Official proquest Official publons Official pubmed Official refseek Official researchgate Official scienceopen Official scopus Official semantic scholar Official ssrn Official web of science Official worldcat Official Publication databases Name Link acs Official asce Official asme Official cambridge Official edp Official elsevier Official emerald Official hindawi Official ieee Official inderscience Official informs Official iop Official ios Official jstor Official mary ann liebert Official mdpi Official nature Official osa Official oxford Official plos Official sage Official science Official scipub Official springer Official taylor \u0026amp; francis Official wiley Official world scientific Official Share databases Name Link academia Official arxiv Official authorea Official engrxiv Official hal Official research square Official techrxiv Official vixra Official Citation \u0026amp; referencing Name Link bibbase Official bibdesk Official biblioscape Official bibme Official bibsonomy Official citation machine Official citavi Official cite fast Official cite this for me Official citeulike Official docear Official endnote Official jabref Official mendeley Official paperpile Official readcube Official refbase Official refdb Official refworks Official zotero Official Journal suggestion Name Link cofactor Official edanz Official elsevier Official endnote Official ieee Official jane Official journal finder Official journal guide Official master journal list Official open journal matcher Official springer Official taylor and francis Official tcs Official Journal validation metrics Name Link academic accelerator Official bioxbio Official jcr Official Journal IF Official manuscript lab Official sjr Official Plagiarism checkers Name Link copyleaks Official cross check Official dupli checker Official grammarly Official ithenticate Official plag scan Official quetext Official turnitin Official Citation netwrok analysis Name Link citation gecko Official citnetexplorer Official connected papers Official research rabbit Official vosviewer Official All in one! Name Link flowcite Official ","date":"2022-09-04T00:00:00Z","image":"https://ktafakkori.github.io/list-of-academic-research-databases-update-2022/post-image_hu950a37ef482bda15ae85fc7c2a44e97a_435546_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/list-of-academic-research-databases-update-2022/","title":"List of academic research databases: Update 2022"},{"content":"Introduction This article can be a comprehensive reference for academics and experts in industrial engineering (IE), supply chain management (SCM), operations research (OR), computer science (CS), machine learning (ML), simulation (SM), data science (DS), and others to get familiar with what is available for machine learning in Python.\nGuide Package capability Description GL General learning DL Deep learning EL Explainable learning PL Probabilistic learning FL Federated learning DSL Distributed learning RL Reinforcement learning RCL Recommendation learning AL Automated learning SPL Specialized learning WM Workflow management GL Packages Package Link annoy Official apache-flink Official apache-singa Official astroml Official backprop Official bayeso Official bentoml Official bigml Official bmfh Official bolt Official brainstorm Official breze Official bytehub Official caffe Official catalyst Official catboost Official chainer Official chefboost Official chemicalx Official cntk Official cogitare Official colossal-ai Official cornac Official couler Official covertree Official crab Official deap Official deep-learning-project-template Official deepchecks Official determined Official digits Official dirty_cat Official dm-haiku Official dtaidistance Official dynet Official edward Official eurybia Official evidently Official faiss Official fastai Official featureforge Official fedot Official fklearn Official flax Official fuku-ml Official gensim Official geomstats Official gradio Official graphlab-create Official hebel Official hub Official mist Official igel Official imbalanced-ensemble Official imbalanced-learn Official implicit Official ivy-core Official jax Official jina Official karateclub Official keras Official ktrain Official lasagne Official lightfm Official lightgbm Official lightwood Official mace Official machine learning Official metric-learn Official milk Official mindsdb Official miraiml Official ml-from-scratch Official ml-model-building Official mlens Official mljar-supervised Official mllib-spark Official mlpack Official mlxtend Official mmlspark Official modal Official mxnet Official neoml Official neon Official neonrvm Official neural-networks-deep-learning Official neural-tangents Official neuraxle Official neurolab Official neuropredict Official nilearn Official nnabla Official numpy-ml Official nupic Official objax Official opfython Official optuna Official optunity Official opytimizer Official orange Official parris Official pattern Official pebl Official pgmpy Official pix2pix Official pomegranate Official prophet Official pybrain Official pydeep Official pyevolve Official pygrid Official pyhsmm Official pylearn2 Official pymilvus Official pyod Official pyspark Official pysyft Official python-recsys Official python-timbl Official pytorch-lightning-bolts Official pytorch-lightning Official pytorch-geometric-temporal Official pytorch-ignite Official lightning Official pytorch Official recommenders Official rep Official restricted-boltzmann-machines Official rexmex Official rgf_python Official river Official scikit-learn-intelex Official scikit-learn Official scikit-multiflow Official shapash Official shapley Official shogun Official simpleai Official skbayes Official sklearn-genetic-opt Official skll Official skorch Official sktime Official spearmint Official stacked_generalization Official statsmodels Official stellargraph Official steppy-toolkit Official steppy Official streamlit Official synthia Official tensorflow-rocm Official tensorflow Official tensorpack Official tflearn Official thampi Official theano Official thinking bayes Official thundergbm Official thundersvm Official topik Official towhee Official tpot Official turicreate Official upgini Official vowpalwabbit Official xcessiv Official xgboost Official xlearn Official xrbm Official DL Packages Package Link blocks Official caffe Official chainer Official chefboost Official cntk Official data-driven-code Official deepy Official dlib Official dm-sonnet Official dynet Official einops Official elegy Official elephas Official fastai Official jina Official keras Official lasagne Official ludwig Official manning Official mindsdb Official mxnet Official neupy Official neupy Official neuraltalk Official neuraltalk2 Official neurolab Official neuron Official nn_builder Official nolearn Official nupic Official paddlepaddle Official pylearn2 Official pytorch Official sequitur Official shogun Official sparkdl Official starspace Official tflearn Official theano Official thinc Official torchbearer Official tresnet Official EL Packages Package Link captum Official cleverhans Official deep-visualization-toolbox Official dowhy Official eli5 Official interpret Official keras-vis Official lime Official lit Official lucid Official mindsdb Official netron Official shap Official shapash Official what-if-tool Official yellowbrick Official PL Packages Package Link edward Official gpytorch Official hmmlearn Official pandas-ta Official pomegranate Official pymc3 Official pyro-ppl Official tensorflow-probability Official FL Packages Package Link crypten Official differential-privacy Official fate Official flwr Official opacus Official pysyft Official seal Official tensorflow-privacy Official tf-encrypted Official DSL Packages Package Link beam Official bentoml Official bigdl Official colossalai Official coremltools Official dask Official deap Official deepspeed Official horovod Official hummingbird Official lightning Official mmdnnn Official onnx Official ray Official tensorflowonspark Official torchserve Official triton Official vespa Official RL Packages Package Link acme Official autonomous-learning-library Official baselines Official catalyst Official chainerrl Official chainerrl Official coax Official d3rlpy Official deeprl Official dopamine-rl Official elf Official garage Official gym Official keras-gym Official keras-rl Official machina-rl Official magent Official mametoolkit Official maze-rl Official mlagents [Official](https://github.com/Unity-Technologies/ml-agents +imitation learning ) mushroom_rl Official open_spiel Official pyqlearning Official pytorch-a2c-ppo-acktr-gail Official pytorch-rl Official reagent Official reaver Official rl-coach Official rlgraph Official rlkit Official rllib Official rlpyt Official slm_lab Official softlearning Official spinningup Official stable-baselines Official stable-baselines3 Official story-baselines Official surreal Official tensorforce Official tf-agents Official tianshou Official trfl Official vel Official RCL Packages Package Link implicit Official lightfm Official recommenders Official scikit-surprise Official spotlight Official AL Packages Package Link nni Official autokeras Official featuretools Official autogluon Official adanet Official model_search Official enas Official SPL Packages input: document, text, or language Package Link allennlp Official bigartm Official bllipparser Official ciphey Official cltk Official colibri-core Official corenlp Official dedupe Official deepke Official deeppavlov Official deeppavlov Official distance Official dl-translate Official drqa Official editdistance Official english-words Official fairseq Official fastnlp Official fasttext Official flair Official flair Official ftfy Official fuzzy wuzzy Official genius Official gensim Official gluonnlp Official gpt-2 Official haystack Official jellyfish Official jieba Official konlpy Official lingvo Official loso Official nalp Official nemo-toolkit Official neuroner Official nlpaug Official nltk Official nut Official opennmt-py Official openprompt Official parlai Official pattern Official pkuseg-python Official polyglot Official promptsource Official pynlpl Official pyss3 Official pystanforddependencies Official pytext-nlp Official pytextrank Official python-frog Official python-ucto Official python-zpar Official quepy Official rasa Official rosetta Official rubrix Official scattertext Official scispacy Official semantic Official sense2vec Official sentence-transformers Official sentencepiece Official snips-nlu Official snorkel Official snowballstemmer Official snownlp Official sockeye Official spacy-transformers Official spacy Official spacy Official spammy Official spark-nlp Official stable-baselines Official stanford-corenlp-python Official stanza Official sumy Official t5 Official tensorflow-text Official textacy Official textblob Official textdistance Official tokenizers Official torchtext Official transformers Official transformers Official vadersentiment Official vocabulary Official xlm Official yalign Official yase Official input: image, video or camera Package Link albumentations Official compreface Official computer-vision-in-action Official de-tr Official deep-high-resolution-net Official deepface Official detecto Official detectron Official detectron2 Official doc2text Official docker-face Official dream-creator Official easyocr Official face-alignment Official face-recognition Official glfw Official gluoncv Official imageai Official imgaug Official imutils Official insightface Official iot-owl Official keras-ocr Official kornia Official learnergy Official lightly Official lightning-bolts Official lucent Official mmdet Official mmf Official mmocr Official moviepy Official neural-dream Official neural-style Official ocrmypdf Official opencv Official openface Official openpose Official openvisionapi Official paddledet Official paddleocr Official paddleseg Official pcv Official pdftabextract Official pillow Official pyslowfast Official pytessarct Official pytorch3d Official pytorchcv Official retinaface Official scikit-image Official simplecv Official simpletk Official tesseract Official tesserocr Official timm Official torchvision Official vigranumpy Official vit-pytorch Official input: network or graph Package Link dgl Official euler Official graph-nets Official graphsage Official networkx Official openke Official torch-geometric Official input: audio, voice, or music Package Link aubio Official deepspeech Official dejavu Official espnet Official librosa Official magenta Official porcupine Official pyaudioanalysis Official pydub Official speechbrain Official speechrecognition Official spleeter Official tts Official input: map, location, or spatial data Package Link folium Official geopandas Official geopy Official prettymaps Official pydeck Official shapely Official input: financial data Package Link alpha_vantage Official backtesting Official backtrader Official crypto-signal Official finmarketpy Official pyalgotrade Official pyfolio Official pyqlib Official ta Official tensortrade Official tf-quant-finance Official yfinance Official zipline Official input: time series data Package Link darts Official gluonts Official prophet Official sktime Official tsfresh Official input: medical data Package Link deepvariant Official dltk Official lifelines Official medicaldetectiontoolkit Official medicalnet Official monai Official niftynet Official input: tabular data Package Link carefree-learn Official deltapy Official pytorch_tabular Official WM Packages Package Link aim Official azureml-sdk Official catalyst Official clearml Official cml Official dvc Official flyte Official metaflow Official mlflow Official pycaret Official sacred Official skaffold Official tensorboard Official tensorboardx Official tensorrt Official visualdl Official wandb Official Notes Some lists are currently available and maintained, which include the following. However, the focus here is on those packages with higher stars (according to GitHub) or popularity. Then, the final purpose is to reclassify packages to help the readers choose them.\n1 2 3 4 5 6 https://github.com/EthicalML/awesome-production-machine-learning https://github.com/josephmisiti/awesome-machine-learning https://github.com/ml-tooling/best-of-ml-python/blob/main/README.md https://github.com/Nyandwi/machine_learning_complete https://github.com/Sahith02/machine-learning-algorithms https://github.com/sorend/awesome-python-machine-learning ","date":"2022-08-20T00:00:00Z","image":"https://ktafakkori.github.io/list-of-machine-learning-packages-in-python-update-2022/post-image-ml-pkg_hud0973c413aa614b3a7812632e826c91d_136828_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/list-of-machine-learning-packages-in-python-update-2022/","title":"List of machine learning packages in Python: Update 2022"},{"content":"Most of the problems in our world are in the form of an MILP.\nAn optimization expert may look for locating facilities by choosing among multiple candidate sites. Then, try to determine the amount of annual production to meet the yearly demand. Such decision-making is at a strategic level, which usually includes an n-year time horizon of impacts. For instance, if we locate facilities in areas with higher traffic congestion, for the next n years, the firm would be responsible for paying penalties for unsustainable transportation or noise pollution due to production processes. Accordingly, it could bring huge costs by making a negligibly different solution.\nOn the other hand, some decisions are tactical, which usually includes master planning flows in a supply chain to meet seasonal or monthly demands. What if such decisions were made wrong? The firm may lose its competitiveness or social responsibility by not meeting demand due to the shortages. Shortages could be due to a lack of proper inventory planning or, in the worst case, disruptive events. So, if we do not consider multiple scenarios, again, huge losses would be imposed.\nAt the operational level, a firm is usually looking to optimize its operations such as routing, scheduling, manufacturing, etc. At this level, the impact of wrong decisions may be negligible, but if we look at the scale and frequency of the operations, we can not ignore possible cumulative losses in the long run. For example, even controlling a system in real-time to reduce the speed of an autonomous vehicle while a human is seen or answering a pool of orders for last-mile delivery can be vital. An intensely robust and reliable system should make these decisions as the losses are usually vast and irreversible! Therefore, exact solutions to logically modeled optimization problems do matter.\nDecomposition-based techniques can alleviate SOME concerns mentioned like: \u0026ldquo;since the problem is NP-hard, the solution time is high, and we can not apply exact methods.\u0026rdquo; These techniques owe to increase the applicability of exact optimization methods but also can be used in heuristic-based methods. Empowered by quantum and cloud computing, this alleviation process can go further. For instance, consider a problem where a brute force method to search all possible \u0026ldquo;yes\u0026rdquo; or \u0026ldquo;no\u0026rdquo; is $2^n$, where n is the number of binary variables. If this problem contains 6 variables, we must search among 64 solutions. If the computer can search for a solution in 1 second, the total CPU time would be 64s. Now consider decomposing this problem to two with 3 variables. Now each of these problems would take $2^3$ (8) seconds to solve. 8+8=16s still is far away from 64s, which is required for the original problem to be solved. That\u0026rsquo;s the central intuition for legendary decomposition-based methods.\nOne of these methods is the Benders Decomposition, proposed by Jacque F. Benders in 1962. In this method, a problem is decomposed into a master problem (leader) (MP) and a subproblem (follower) (SP). MP gives the subproblem (or correctly the dual version of the subproblem) some integer solutions, by which the SP finds some dual values and helps the MP to revise its solutions. Its basic version is applicable to large scale LPs and MILPs, but there are more advanced versions which can be applied to NLPs, MINLPs and even pure IPs.\nThe figure below shows how a problem\u0026rsquo;s upper and lower bounds obtained by MP and SP get close to each other after several iterations of this algorithm.\nIf you are interested in fully understanding how the basic version of the benders decomposition works, you may purchase our tutorial! Visit this page: Tutorials. This will empower you to build scalable models.\n","date":"2022-08-03T00:00:00Z","image":"https://ktafakkori.github.io/benders-decomposition-algorithm-why-is-it-important/post-image_hued055a20ee3db191ddaaa68e27d3189d_403115_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/benders-decomposition-algorithm-why-is-it-important/","title":"Benders decomposition algorithm: Why is it important?"},{"content":"Introduction This article can be a comprehensive reference for academics and experts in industrial engineering (IE), supply chain management (SCM), operations research (OR), computer science (CS), machine learning (ML), simulation (SM), data science (DS), and others to get familiar with what is available for optimization in Python.\nGuide Package capability Description MINLP Mixed integer nonlinear programming MIQP Mixed integer quadratic programming MILP Mixed integer linear programming NLP Nonlinear programming IP Integer programming LP Linear programming CP Constraint programming GPP General purpose programming MOP Multi-criteria/objective programming MINLP+MIQP+MILP+NLP+IP+LP Packages Package Link casadi Official feloopy Official gekko Official knitro Official lindo Official midaco Official naginterfaces Official octeract Official optalg Official optmod Official pydrake Official pyomo Official pyscipopt Official xpress Official MIQP+MILP+IP+LP Packages Package Link copt Official cplex Official docplex Official gurobipy Official highs Official localsolver Official mosek Official optlang Official rsome Official sasoptpy Official qpsolvers Official MILP+IP+LP Packages Package Link cvxopt Official cvxpy Official cylp Official flowty Official linopy Official lpsolve55 Official mindoptpy Official mip Official ortools Official picos Official pulp Official pymprog Official swiglpk Official NLP+LP Packages Package Link acadopy Official acados Official cyipopt Official dymos Official gpkit Official iminuit Official nlopt Official nlpy Official openmdao Official openopt Official polyopt Official pyipopt Official pyopt Official scipy Official trustregion Official worhp Official CP Packages Package Link cplex Official cpmpy Official gecode-python Official kalis Official minizinc Official optapy Official ortools Official python-constraint Official z3-solver Official GPP Packages Package Link arm-mango Official ax Official bayesian-optimization Official bayesianevolution Official bayeso Official bayesopt Official black-box Official bolib Official cma Official cmaes Official cobyqa Github cuopt Official deap Official dfoalgos Official dfogn Official dlib Official evolopy Official evoopt Official evostra Official feloopy Official freelunch Official gaft Official geneticalgorithm Official goptpy Github gradient-free-optimizers Github gyopt Official hebo Official heuristic_optimization Official hpbandster Official hyperopt Official inspyred Official mealpy Official mipego Official moptipy Official mystic Official nevergrad Official niapy Official oasis Official optuna Official optuner Official opytimizer Official pagmo Official pdfo Official platypus Official prodyn Official proxmin Official psopt Official psopy Official py-bobyqa Official pydogs Official pygmo Official pygpgo Official pymoo Official pyopus Official pypesto Official pyriad Official pysmac Official pysot Official pyswarms Official pymetaheuristic Official qiskit-optimization Official rapids-NeurIPS Official ray Official rbfopt Official scikit-opt Official scikit-optimize Official simanneal Official simple Official solidpy Official spearmint Official spotpy Official ssb-optimize Official swarm-cg Github swarmlib Official swarmpackagepy Official tgo Official turbo-NeurIPS Official turbo Official ultraopt GitHub yabox GitHub zoofs GitHub zoopt GitHub MOP Packages Package Link pymultiobjective Official pydecision Official Notes 1- If you are having trouble while installing via !pip install \u0026lt;PACKAGE\u0026gt; (in-line code) or pip install \u0026lt;PACKAGE\u0026gt; (terminal code), you may use the following piece of code. Also, please be aware that some of the introduced packages require installing software or downloading and compiling a copy of their binary files to be imported into Python. Therefore, some of them are not easily pip installable.\n1 2 3 4 5 6 7 8 9 10 11 import pip #Function: def install(package): if hasattr(pip, \u0026#39;main\u0026#39;): pip.main([\u0026#39;install\u0026#39;, package]) else: pip._internal.main([\u0026#39;install\u0026#39;, package]) #Example: install(\u0026#39;pyomo\u0026#39;) 2- There are some benchmarkig tools and websites, which are introduced as follows:\nBenchmark Link humpday Official pycutest Official Mittelmann Official ","date":"2022-07-27T00:00:00Z","image":"https://ktafakkori.github.io/list-of-optimization-packages-in-python-update-2023/post-image_hu866100f6c3e1f4552644383d10008314_116926_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/list-of-optimization-packages-in-python-update-2023/","title":"List of optimization packages in Python: Update 2023"},{"content":"Step 1. Defining the optimization model The optimization model can be defined by a Python function. The inputs to this function would be the sets, parameters, and variables. The output would be the symbolic objective(s) and constraints.\nFor instance, the following optimization model maximizes the net present value of executing improvement projects on some facilities. Selecting project i for facility j is denoted by binary variable x, and determining the budget for these improvements is represented by positive variable y. The parameters c and a denote the net present value and investment required for each project for each facility, respectively. The total budget for the headquarter to implement these improvement projects is indicated by the parameterb. Finally, the sets J and I represent the number of facilities and projects, respectively. Considering that the total amount of investment for all facilities should be lower than the entire budget available, to maximize the total net present value, the optimization model can be created as follows:\n1 2 3 4 5 6 7 import itertools as it def model(I,J,x,y): objs = {0: sum(c[j,i]*x[j,i] for j,i in it.product(J,I))} cons = {0: {j: sum(a[j,i]*x[j,i] for i in I) \u0026lt;= y[j] for j in J}, 1: {0: sum(y[j] for j in J) \u0026lt;= b}} return objs, cons Notably, this part is independent of any optimization interface used. Also, since I am using itertools module for constraints or summations with multiple indices, we need to import it.\nStep 2. Using the optimization interface In this next step, we configure the optimization interface used to generate a solver-friendly intermediary file (e.g., .mps or .lp). Usually, the most critical setting in this step is to create sets and variables and feed them to the model previously created. Again, I use a Python function to adjust the interface:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import pyomo.environ as op def interface(): #Used for creating the environment and naming it in Pyomo m = op.ConcreteModel(name) #Used for defining sets in PYOMO m.I = op.Set(initialize=I) m.J = op.Set(initialize=J) #Used for defining variables in PYOMO m.x = op.Var(m.J,m.I, domain=op.Binary) m.y = op.Var(m.J, domain=op.PositiveReals) #Used for feeding created sets and variables to the model (general) objs, cons = model(m.I,m.J,m.x,m.y) #Used for defining objective(s) in PYOMO m.OBJ = op.Objective(expr=objs[0],sense=op.maximize) #Used for defining constraints in PYOMO m.constraint = op.ConstraintList() for keys1 in cons: for keys2 in cons[keys1]: m.constraint.add(expr=cons[keys1][keys2]) return m Step 3. Adjusting the solver In this next step, we feed the interface-generated file to a solver. Since no solver comes with PYOMO pre-installed, I use online solvers from the NEOS Server. For instance, as the proposed model is MILP, I use CPLEX to solve it. Moreover, I consider three inputs for a solver, showmodel, solvemodel, and showresult. If each set to true, the corresponding task would be implemented. Showing the model is suitable in the validation stage. But, we usually do not need it during sensitivity analysis. Solving the model is always required unless we make our optimization model with try and error and want to check its overall look and feel. Finally, showing the result is helpful for validation or sensitivity analysis.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import os def solve(showmodel=True,solvemodel =True,showresult =True): #Used for creating the solver-friendly files m = interface() #Used for validating the model if showmodel: print(\u0026#34;Model:\u0026#34;,m) #Used for calling solvers in PYOMO if solvemodel: os.environ[\u0026#39;NEOS_EMAIL\u0026#39;] = \u0026#39;username@email.com\u0026#39; solver_manager = op.SolverManagerFactory(\u0026#39;neos\u0026#39;) results = solver_manager.solve(m, solver = \u0026#34;cplex\u0026#34;) #Used for validating the model if showresult: print(results) op.display(m) return m Step 4. Defining or feeding datasets In this next step, we need data. It is common to create data randomly to stress test a model. However, one should pay attention to the consequences. The consequences can be WRONG infeasibility alerts, numerical INSTABILITY in the solving process, etc. It is better to understand your data and the relationship between its elements. For instance, if you are randomly creating demand values, the capacity of your facilities should not be lower. Overall, some parameter tuning is required EVEN IF you are testing your model on arbitrarily generated datasets. If the datasets are from real-world applications, parameter tuning is STILL needed. For instance, try not to feed your model with VERY LARGE or SMALL values. The data for the optimization model introduced in step 1 is generated as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np #Used for reproducibility of the results: np.random.seed(10) #Name of the model: name = \u0026#34;Facilities Investment Planning\u0026#34; #Cost of implementing each project for each facility a = np.random.uniform(10, 20, (5, 6)) # Net present value (benefit) of implementing each project for each c = np.random.uniform(100, 300, (5, 6)) #Total budget available b = 80 #Set of facilities J = range(a.shape[0]) #Set of improvement projects I = range(a.shape[1]) Step 5. Implementing the model In this next step, we need to implement the model to see if it generates feasible, logical, and optimal solutions.\n1 2 #Implementation after feeding data m = solve(showmodel=True,solvemodel =True,showresult =True) Step 6. Visualizing the results Understanding the obtained results is easy for one who has developed the model. However, what if these results will be presented to an audience without involvement in the process? In this step, one should try to find the BEST plots and figures to visualize the data. In some cases, generating a TABLE is even enough. For this simple optimization problem, I visualize the binary variable x using imshow from matplotlib.pyplot module in Python and a simple bar chart to show the investment amount for each facility. I also can visualize the parameters to see if the relationship between the inputs and outputs of a model is valid and logical.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import matplotlib.pyplot as plt plt.figure(1) x = np.zeros(shape=(5, 6)) for j in J: for i in I: if m.x.get_values()[(j,i)]==1: x[j,i]=1 plt.title(\u0026#34;Investment Portfolio\u0026#34;) plt.xlabel(\u0026#34;Projects\u0026#34;) plt.ylabel(\u0026#34;Facilities\u0026#34;) plt.imshow(x) plt.colorbar() plt.figure(2) plt.title(\u0026#34;Investment Amount\u0026#34;) plt.xlabel(\u0026#34;Facilities\u0026#34;) plt.ylabel(\u0026#34;Amount of investment\u0026#34;) plt.bar(list(m.y.get_values().keys()),list(m.y.get_values().values())) plt.figure(3) plt.title(\u0026#34;Cost\u0026#34;) plt.xlabel(\u0026#34;Projects\u0026#34;) plt.ylabel(\u0026#34;Facilities\u0026#34;) plt.imshow(a) plt.colorbar() plt.figure(4) plt.title(\u0026#34;Benefit\u0026#34;) plt.xlabel(\u0026#34;Projects\u0026#34;) plt.ylabel(\u0026#34;Facilities\u0026#34;) plt.imshow(c) plt.colorbar() Step 7. Sensitivity analysis After validating the model, we need robustness checking. That means, how are the results robust to changes? What is the trend in changes of outputs while inputs are changed? This step is the most important as it introduces the model\u0026rsquo;s behavior for other cases. It is also a simple method to see the effect of uncertainties in the values of the parameters. It even empowers the whole neural networks we see in machine learning! Why? The changes in input parameters (e.g., number of layers or number of neurons in each layer) can affect accuracy (the objective). So it can help one determine the best structure for a neural network for the training dataset, to then be applied to the test dataset! A simple sensitivity analysis is to increase or decrease the values of parameters one-by-one to see their effect on the objective (response), as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 S = range(5) aa = a.copy() for s in S: a = a*(1+s/len(S)) m = solve(showmodel=False,solvemodel=True,showresult=False) print(op.value(m.OBJ)) a = aa bb = b.copy() S = range(5) for s in S: b = b*(1-s/len(S)) m = solve(showmodel=False,solvemodel=True,showresult=False) print(op.value(m.OBJ)) b = bb cc = c.copy() S = range(5) for s in S: c = c*(1-s/len(S)) m = solve(showmodel=False,solvemodel=True,showresult=False) print(op.value(m.OBJ)) c = cc The optimization pipeline Based on the discussed steps, a generic optimization pipeline is as follows:\nDefining the optimization model Using the optimization interface Adjusting the solver Defining or feeding datasets Implementing the model Visualizing the results Sensitivity analysis Conclusion In this article, I proposed a working example for optimization with PYOMO in Python. Then, introduced an optimization pipeline that is generic and can be applied when an operations research scientist is modeling, solving, and analyzing an optimization problem. If the content was helpful, consider supporting the project FELOOP or sharing the content with your colleagues and friends! Also, feel free to contact me if there are any questions.\n","date":"2022-07-15T00:00:00Z","image":"https://ktafakkori.github.io/optimization-by-pyomo-in-python-a-complete-working-example/post-image_hu3b40f16f771714ac8dadee21d0875989_110425_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/optimization-by-pyomo-in-python-a-complete-working-example/","title":"Optimization by PYOMO in Python: A complete working example"},{"content":"In this short article, I am sharing my opinion and explaining how operations research and machine learning are interrelated.\nSuppose a continuous facility location problem with squared Euclidean distance as follows:\n$$ \\begin{aligned} \u0026amp; \\min \u0026amp;\u0026amp; \\sum\\limits_{t = 1}^{\\left| T \\right|}w_t\\lVert\\bf{g}-\\bf{b_t}\\rVert^2 \\ \u0026amp;\u0026amp;\u0026amp; {\\bf{g}} \\in \\mathbb{R^{|U|}} \\ \\end{aligned} $$\nIf you remember, the distance (similarity) measure applied in this facility location problem helps find the location of emergency facilities. That means we want to find the location of a single facility that can serve T facilities as fast as possible. As seen, g is in bold format and not indexed. Being in the boldface means that g has other dimensions (a common mathematical notation). For example, since this is a continuous facility location problem, we need to find $g_1$ and $g_2$ indicating $x$ and $y$ of a new facility in a two-dimensional space. $b_t$, which is also in bold format, denotes x and y for the t-th existing facility. Finally, $w_t$ is the weight of from-to flows between existing and the new facilities (a similarity indicator). Now consider the following optimization problem:\n$$ \\begin{aligned} \u0026amp; \\min \u0026amp;\u0026amp; \\sum\\limits_{t = 1}^{\\left| T \\right|}w_t\\lVert g_t-b_t \\rVert^2 \\ \u0026amp;\u0026amp;\u0026amp; g_t \\in \\mathbb{R} \\ \\end{aligned} $$\nIn this optimization problem, we wish to find values $g_t$ close to $b_t$, which are not bold, meaning that they have no other dimensions. But, $g$ is now indexed by $t$. Contrary to the previous model, we need to find the location of $t$ new facilities close to the $t$ existing facilities. If we do not consider some extra constraints, $g_t$ would become equal to $b_t$. This means we establish all $t$ new facilities on the existing $t$ facilities to reduce their distance.\nIn machine learning (specifically supervised learning), existing facilities are represented by data points (observations), and $g_t$ is the location of new $t$ data points, which we wish to be close to $b_t$ (existing facilities). However, without constraints, the machine does not learn anything! We want to find a single function (instead of a single facility) that can be applied to all data points. Accordingly, the optimization model becomes:\n$$ \\begin{aligned} \u0026amp; \\min \u0026amp;\u0026amp; \\sum\\limits_{t = 1}^{\\left| T \\right|}w_t\\lVert g_t-b_t \\rVert^2 \\ \u0026amp;\u0026amp;\u0026amp; g_t = y_t \\quad \\forall t \\in T \\ \u0026amp;\u0026amp;\u0026amp; y_t = \\sum_{i=1}^{|I|} a_{ti}x_i + z \\quad \\forall t \\in T \\ \u0026amp;\u0026amp;\u0026amp; g_t, y_t, x_i, z \\in \\mathbb{R} \\ \\end{aligned} $$\nwhere the first and second constraints represent post- and pre-activation functions, respectively.\nThis optimization problem is called linear regression, the basis for all machine learning models and algorithms! Note that $w_t$ here should equal one (or better 1/T) as we do not want to make a biased model!\n","date":"2022-07-15T00:00:00Z","image":"https://ktafakkori.github.io/the-root-of-similarities-between-supervised-learning-and-operations-research/post-image_hu1a0c7a68df5af7e878a06d9f5f7de432_432405_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/the-root-of-similarities-between-supervised-learning-and-operations-research/","title":"The root of similarities between supervised learning and operations research"},{"content":"For some reason, a meta-heuristic algorithm is not working properly for a problem, or it doesn\u0026rsquo;t seem easy to apply! It abruptly generates impermissible solutions!\nIn this short article, I offer a piece of advice on applying a meta-heuristic (heuristic optimization) algorithm for an optimization problem.\nSome simple optimization problems can directly be coded in a meta-heuristic algorithm. Those include unconstrained ones with some upper and lower bounds on decision variables. These types of problems are primarily seen in the machine learning field.\nBut, many constraints are usually considered when an optimization expert develops a model for a real-world phenomenon such as a supply chain network. These types of models can not be directly encoded or decoded with simple random vectors.\nFor instance, consider a traveling salesman problem. In the mathematical formulation of this optimization problem, many binary variables represent moves from one node to the other. Can we consider these binary variables directly as a variable (data structure) in a meta-heuristic algorithm? The answer is no!\nWhy? Because random variations of these binary variables through iterations can lead to some infeasible or illogical solutions! For instance, if we code a random vector like [1.2,1.5,2.4,3.5,5.6] to show the sequence of visits for five cities (considering a range of random values in the interval [1,6]), then decode it by a rounding method, it can lead to [1, 1, 2, 3, 5] meaning that city #4 is not visited at all. Meanwhile, city #1 is being visited twice!\nSo what to do? The most important thing is to look for constraints with a positive right-hand side (not zeros and ones) and types of data to produce.\nFor instance, are we looking for a sequence? (e.g., in scheduling or routing), are we willing to find a positive value? (e.g., price), are we interested in answering by yes and no? (e.g., finding establishment locations for facilities or items to put in a knapsack), or are we looking for some integer numbers (e.g., the number of robots)?\nThe answer to the above questions leads to different solution encoding and decoding (representation) schemes. For instance, if we are looking for a sequence, the common method is using random key representation. In this method, we generate a random vector like this: [0.95,0.35,0.46,0.57,0.34] (considering a range of random values in the interval [0,1]), then find the position of the numbers by the smallest to the largest sorting scheme. The position of the smallest number is 5, then 5 is the first city to visit! Then, the next would be 2, and 2 would be the next city to visit, and so on. The encoded solution is decoded to [5,2,3,4,1]. Look how we can avoid generating illegal solutions! Variating the encoded values by variation operators can lead to another vector such as [0.45,0.64,0.36,0.17,0.84], eliminating the need for extra repairing or penalty methods, and the solution still remains feasible and logical.\nIn some cases, such as a knapsack problem, we must select some items while meeting the knapsack\u0026rsquo;s capacity. Selecting among 5 items can be encoded by a simple vector like [0.3,0.45,0.7,0.6,0.4]. Then rounding to the closest integer would result in a decoded vector [0,0,1,1,0], meaning that only items 3 and 4 go into the knapsack! But what if their weights (in total) are larger than the knapsack\u0026rsquo;s capacity? Should we consider penalties when the weight of items becomes larger than the capacity? It is an easy way to avoid infeasible solutions. But one can also apply other heuristics (e.g., first-fit or best-fit) to avoid such penalization methods.\nIn summary, to apply a meta-heuristic, you should check if the optimization problem is REALLY constrained or not. Can we still generate feasible values without considering constraints? (in some cases, it can happen!). If constraints can not be easily neglected, which of the above questions apply to the decisions? Can we define proper data structures to undergo variation operators of a meta-heuristic algorithm?\nOverall, one should look for the decisions to be made regarding constraints when applying a meta-heuristic algorithm, then look for representation methods, repairing operators, heuristic operators, special algorithm dependent operators, CORRECT penalty functions, etc., to fix any illogical or infeasible solution.\nThe figure below can help you to understand the differences between exact and heuristic optimization methods in coding a TSP problem:\n","date":"2022-07-14T00:00:00Z","image":"https://ktafakkori.github.io/avoiding-mistakes-in-applying-meta-heuristic-algorithms/post-image-ms-ma_hufab9525c2035d05fa9cc5f2a843bc05e_32426_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/avoiding-mistakes-in-applying-meta-heuristic-algorithms/","title":"Avoiding mistakes in applying meta-heuristic algorithms"},{"content":"Welcome to FELOOP!\nI am happy to share that I am initiating one of my personal projects called FELOOP (an abbreviation for FEasible, LOgical, and OPtimal) through this blog!\nIn this project, I will propose and share multiple approaches, models, algorithms, or codes for enthusiasts in the field of operations research (OR), computer science (CS), and data science (DS). In this blog, everything, even the most complex academic and business problems, theories, methodologies, and case studies, will be discussed in simple words, making it easy to follow for the audience. Therefore, please ensure to follow the updates and share the contents with your colleagues and friends!\nAlso, feel free to let me know your experience with the user interface of this blog.\n","date":"2022-07-13T00:00:00Z","image":"https://ktafakkori.github.io/introducing-feloop/post-image_hu951bce298efbc427281f1a109667eb25_54994_120x120_fill_box_smart1_3.png","permalink":"https://ktafakkori.github.io/introducing-feloop/","title":"Introducing FELOOP"}]